{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to get started with Word2Vec — and then how to make it work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is closely adapted from this awesome blog article from Kavita Ganesan: https://medium.freecodecamp.org/how-to-get-started-with-word2vec-and-then-how-to-make-it-work-d0a2fca9dad3.\n",
    "As the title says, we'll learn how to use the Gensim implementation of Word2Vec and actually get it to work. [...] Getting it to work and obtaining useable results depends, as Kavita points out, on the well set-up combination of two things: (1) your input data and (2) your parameter settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we start with our imports and get logging established:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tobias\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# imports needed and logging\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will work on a dataset from Quora, provided in the scope of the \"Insincere Questions Classification\" on Kaggle. It contains sincere and insincere questions; an insincere question is defined as a question intended to make a statement rather than look for helpful answers. The training data includes the question that was asked, and whether it was identified as insincere (target = 1). The ground-truth labels contain some amount of noise: they are not guaranteed to be perfect. (from: https://www.kaggle.com/c/quora-insincere-questions-classification/data)\n",
    "\n",
    "For working on the data, let's assume we have downloaded and unzipped the dataset (from the source given above), which contains one train.csv and one test.csv table with the textual data and labels.\n",
    "\n",
    "As the dataset is very big, we can create a shortened file with the first few thousand entries at first (we can still use the entire files for the final version of our Training later on). To do this, we need to open the folder containing the train.csv file in a Terminal window (Linux) or a comparable command window that simulates a Linux Terminal in Windows (e.g. the GitLab prompt, or Cygwin). We can then extract the first few thousand lines of train.csv into a shortened file with this Linux command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "head -n NUMBEROFLINES file.csv > mynewfile.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a closer look at our data below by printing the first line(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First rows of train_short.csv:  [['qid', 'question_text', 'target'], ['00002165364db923c7e6', 'How did Quebec nationalists see their province as a nation in the 1960s?', '0'], ['000032939017120e6e44', 'Do you have an adopted dog, how would you encourage people to adopt and not shop?', '0'], ['0000412ca6e4628ce2cf', 'Why does velocity affect time? Does velocity affect space geometry?', '0'], ['000042bf85aa498cd78e', 'How did Otto von Guericke used the Magdeburg hemispheres?', '0'], ['0000455dfa3e01eae3af', 'Can I convert montra helicon D to a mountain bike by just changing the tyres?', '0'], ['00004f9a462a357c33be', 'Is Gaza slowly becoming Auschwitz, Dachau or Treblinka for Palestinians?', '0'], ['00005059a06ee19e11ad', 'Why does Quora automatically ban conservative opinions when reported, but does not do the same for liberal views?', '0'], ['0000559f875832745e2e', 'Is it crazy if I wash or wipe my groceries off? Germs are everywhere.', '0'], ['00005bd3426b2d0c8305', 'Is there such a thing as dressing moderately, and if so, how is that different than dressing modestly?', '0']]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def csv_to_list_raw(filename):\n",
    "    sents = []\n",
    "    with open(filename, newline='', encoding='utf-8') as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in csvreader:\n",
    "            sents.append(row)\n",
    "    return sents\n",
    "\n",
    "train_file = 'train_short.csv'\n",
    "train_raw = csv_to_list_raw(train_file)\n",
    "print (\"First rows of train_short.csv: \", train_raw[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll adapt this function a bit now. Let's also do a mild pre-processing of the text using gensim.utils.simple_preprocess (row[1]). We only use the question text itself, and leave the labels whether a question was \"insincere\" or not aside (being not the primary scope of this exercise here).\n",
    "The simple_preprocess function does some basic pre-processing such as tokenization, lowercasing, and so on and returns back a list of tokens (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First rows of train_short.csv:  [['question_text'], ['how', 'did', 'quebec', 'nationalists', 'see', 'their', 'province', 'as', 'nation', 'in', 'the'], ['do', 'you', 'have', 'an', 'adopted', 'dog', 'how', 'would', 'you', 'encourage', 'people', 'to', 'adopt', 'and', 'not', 'shop'], ['why', 'does', 'velocity', 'affect', 'time', 'does', 'velocity', 'affect', 'space', 'geometry'], ['how', 'did', 'otto', 'von', 'guericke', 'used', 'the', 'magdeburg', 'hemispheres'], ['can', 'convert', 'montra', 'helicon', 'to', 'mountain', 'bike', 'by', 'just', 'changing', 'the', 'tyres'], ['is', 'gaza', 'slowly', 'becoming', 'auschwitz', 'dachau', 'or', 'treblinka', 'for', 'palestinians'], ['why', 'does', 'quora', 'automatically', 'ban', 'conservative', 'opinions', 'when', 'reported', 'but', 'does', 'not', 'do', 'the', 'same', 'for', 'liberal', 'views'], ['is', 'it', 'crazy', 'if', 'wash', 'or', 'wipe', 'my', 'groceries', 'off', 'germs', 'are', 'everywhere'], ['is', 'there', 'such', 'thing', 'as', 'dressing', 'moderately', 'and', 'if', 'so', 'how', 'is', 'that', 'different', 'than', 'dressing', 'modestly']]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def csv_to_list(filename):\n",
    "    sents = []\n",
    "    with open(filename, newline='', encoding='utf-8') as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in csvreader:\n",
    "            row_text = gensim.utils.simple_preprocess(row[1])\n",
    "            sents.append(row_text)\n",
    "    return sents\n",
    "\n",
    "train_file = 'train_short.csv'\n",
    "sentences = csv_to_list(train_file)\n",
    "print (\"First rows of train_short.csv: \", sentences[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, from Radim Řehůřek, the creator of the gensim package, an explanation of the parameters we're going to set for creating the model (https://radimrehurek.com/gensim/models/word2vec.html):\n",
    "\n",
    "- sentences (iterable of iterables, optional) – The sentences iterable can be simply a list of lists of tokens, but for larger corpora, consider an iterable that streams the sentences directly from disk/network. See BrownCorpus, Text8Corpus or LineSentence in word2vec module for such examples. See also the tutorial on data streaming in Python. If you don’t supply sentences, the model is left uninitialized – use if you plan to initialize it in some other way.\n",
    "corpus_file (str, optional) – Path to a corpus file in LineSentence format. You may use this argument instead of sentences to get performance boost. Only one of sentences or corpus_file arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
    "- size (int, optional) – Dimensionality of the word vectors.\n",
    "- window (int, optional) – Maximum distance between the current and predicted word within a sentence.\n",
    "- min_count (int, optional) – Ignores all words with total frequency lower than this.\n",
    "- workers (int, optional) – Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "\n",
    "The input parameters are of the following types:\n",
    "word (str) - the word we are examining\n",
    "count (int) - the word’s frequency count in the corpus\n",
    "min_count (int) - the minimum count threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-28 00:22:06,092 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-03-28 00:22:06,111 : INFO : collecting all words and their counts\n",
      "2019-03-28 00:22:06,114 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-03-28 00:22:06,252 : INFO : PROGRESS: at sentence #10000, processed 120880 words, keeping 14878 word types\n",
      "2019-03-28 00:22:06,368 : INFO : PROGRESS: at sentence #20000, processed 242134 words, keeping 21567 word types\n",
      "2019-03-28 00:22:06,483 : INFO : PROGRESS: at sentence #30000, processed 363093 words, keeping 26675 word types\n",
      "2019-03-28 00:22:06,593 : INFO : PROGRESS: at sentence #40000, processed 484276 words, keeping 30912 word types\n",
      "2019-03-28 00:22:06,713 : INFO : PROGRESS: at sentence #50000, processed 605073 words, keeping 34597 word types\n",
      "2019-03-28 00:22:06,834 : INFO : PROGRESS: at sentence #60000, processed 726181 words, keeping 38005 word types\n",
      "2019-03-28 00:22:06,947 : INFO : PROGRESS: at sentence #70000, processed 848209 words, keeping 41047 word types\n",
      "2019-03-28 00:22:07,059 : INFO : PROGRESS: at sentence #80000, processed 969844 words, keeping 43904 word types\n",
      "2019-03-28 00:22:07,164 : INFO : PROGRESS: at sentence #90000, processed 1091654 words, keeping 46608 word types\n",
      "2019-03-28 00:22:07,268 : INFO : collected 49058 word types from a corpus of 1213051 raw words and 100000 sentences\n",
      "2019-03-28 00:22:07,272 : INFO : Loading a fresh vocabulary\n",
      "2019-03-28 00:22:07,537 : INFO : min_count=1 retains 49058 unique words (100% of original 49058, drops 0)\n",
      "2019-03-28 00:22:07,538 : INFO : min_count=1 leaves 1213051 word corpus (100% of original 1213051, drops 0)\n",
      "2019-03-28 00:22:07,829 : INFO : deleting the raw counts dictionary of 49058 items\n",
      "2019-03-28 00:22:07,833 : INFO : sample=0.001 downsamples 47 most-common words\n",
      "2019-03-28 00:22:07,833 : INFO : downsampling leaves estimated 907557 word corpus (74.8% of prior 1213051)\n",
      "2019-03-28 00:22:08,100 : INFO : estimated required memory for 49058 words and 150 dimensions: 83398600 bytes\n",
      "2019-03-28 00:22:08,108 : INFO : resetting layer weights\n",
      "2019-03-28 00:22:09,525 : INFO : training model with 10 workers on 49058 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-03-28 00:22:10,634 : INFO : EPOCH 1 - PROGRESS: at 44.59% examples, 400530 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-28 00:22:11,543 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-28 00:22:11,575 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-28 00:22:11,585 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-28 00:22:11,605 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-28 00:22:11,612 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-28 00:22:11,629 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-28 00:22:11,639 : INFO : EPOCH 1 - PROGRESS: at 97.99% examples, 441838 words/s, in_qsize 3, out_qsize 1\n",
      "2019-03-28 00:22:11,644 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 00:22:11,661 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 00:22:11,666 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 00:22:11,669 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 00:22:11,670 : INFO : EPOCH - 1 : training on 1213051 raw words (907859 effective words) took 2.0s, 444084 effective words/s\n",
      "2019-03-28 00:22:12,782 : INFO : EPOCH 2 - PROGRESS: at 49.55% examples, 428485 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-28 00:22:13,642 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-28 00:22:13,646 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-28 00:22:13,665 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-28 00:22:13,669 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-28 00:22:13,675 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-28 00:22:13,690 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-28 00:22:13,693 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 00:22:13,701 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 00:22:13,708 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 00:22:13,731 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 00:22:13,734 : INFO : EPOCH - 2 : training on 1213051 raw words (908247 effective words) took 2.0s, 454365 effective words/s\n",
      "2019-03-28 00:22:14,835 : INFO : EPOCH 3 - PROGRESS: at 47.10% examples, 409893 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-28 00:22:15,675 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-28 00:22:15,719 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-28 00:22:15,730 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-28 00:22:15,746 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-28 00:22:15,755 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-28 00:22:15,760 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-28 00:22:15,767 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 00:22:15,775 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 00:22:15,785 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 00:22:15,793 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 00:22:15,796 : INFO : EPOCH - 3 : training on 1213051 raw words (907498 effective words) took 2.0s, 453751 effective words/s\n",
      "2019-03-28 00:22:16,890 : INFO : EPOCH 4 - PROGRESS: at 48.74% examples, 427495 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-28 00:22:17,790 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-28 00:22:17,800 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-28 00:22:17,808 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-28 00:22:17,815 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-28 00:22:17,818 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-28 00:22:17,819 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-28 00:22:17,822 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 00:22:17,823 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 00:22:17,824 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 00:22:17,825 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 00:22:17,826 : INFO : EPOCH - 4 : training on 1213051 raw words (907447 effective words) took 2.0s, 461298 effective words/s\n",
      "2019-03-28 00:22:18,915 : INFO : EPOCH 5 - PROGRESS: at 45.39% examples, 399995 words/s, in_qsize 15, out_qsize 4\n",
      "2019-03-28 00:22:19,795 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-28 00:22:19,838 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-28 00:22:19,844 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-28 00:22:19,845 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-28 00:22:19,850 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-28 00:22:19,862 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-28 00:22:19,872 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 00:22:19,875 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 00:22:19,880 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 00:22:19,883 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 00:22:19,889 : INFO : EPOCH - 5 : training on 1213051 raw words (907793 effective words) took 2.0s, 453580 effective words/s\n",
      "2019-03-28 00:22:19,891 : INFO : training on a 6065255 raw words (4538844 effective words) took 10.4s, 438001 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary and train model\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    size=150,\n",
    "    window=10,\n",
    "    min_count=1,\n",
    "    workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building the vocabulary, we just need to call train(...) to start training the Word2Vec model. Behind the scenes we are actually training a simple neural network with a single hidden layer. But we are actually not going to use the neural network after training. Instead, the goal is to learn the weights of the hidden layer. These weights are essentially the word vectors that we’re trying to learn.\n",
    "\n",
    "In short, again from Radim ():\n",
    "With the \"train\" method, we'll \"update the model’s neural weights from a sequence of sentences\".\n",
    "The parameter \"epochs\" is defined as follows:\n",
    "- epoch (int) – Number of iterations (epochs) over the corpus.\n",
    "\n",
    "Training on the Word2Vec OpinRank dataset takes about 10–15 minutes. so please be patient while running your code on this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-28 00:25:42,622 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-03-28 00:25:42,624 : INFO : training model with 10 workers on 49058 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-03-28 00:25:43,769 : INFO : EPOCH 1 - PROGRESS: at 47.10% examples, 393751 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-28 00:25:44,618 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-28 00:25:44,636 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-28 00:25:44,649 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-28 00:25:44,652 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-28 00:25:44,655 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-28 00:25:44,671 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-28 00:25:44,678 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 00:25:44,684 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 00:25:44,708 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 00:25:44,715 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 00:25:44,719 : INFO : EPOCH - 1 : training on 1213051 raw words (907746 effective words) took 2.0s, 446836 effective words/s\n",
      "2019-03-28 00:25:45,814 : INFO : EPOCH 2 - PROGRESS: at 35.50% examples, 311544 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-28 00:25:46,821 : INFO : EPOCH 2 - PROGRESS: at 75.84% examples, 337464 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-28 00:25:47,210 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-28 00:25:47,214 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-28 00:25:47,219 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-28 00:25:47,232 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-28 00:25:47,237 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-28 00:25:47,267 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-28 00:25:47,278 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 00:25:47,285 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 00:25:47,293 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 00:25:47,306 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 00:25:47,312 : INFO : EPOCH - 2 : training on 1213051 raw words (908135 effective words) took 2.5s, 358857 effective words/s\n",
      "2019-03-28 00:25:48,409 : INFO : EPOCH 3 - PROGRESS: at 46.29% examples, 405917 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-28 00:25:49,434 : INFO : EPOCH 3 - PROGRESS: at 88.99% examples, 392009 words/s, in_qsize 14, out_qsize 0\n",
      "2019-03-28 00:25:49,511 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-28 00:25:49,539 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-28 00:25:49,542 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-28 00:25:49,546 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-28 00:25:49,549 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-28 00:25:49,549 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-28 00:25:49,557 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 00:25:49,562 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 00:25:49,570 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 00:25:49,579 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 00:25:49,582 : INFO : EPOCH - 3 : training on 1213051 raw words (907124 effective words) took 2.2s, 411633 effective words/s\n",
      "2019-03-28 00:25:50,653 : INFO : EPOCH 4 - PROGRESS: at 37.98% examples, 338748 words/s, in_qsize 20, out_qsize 2\n",
      "2019-03-28 00:25:51,676 : INFO : EPOCH 4 - PROGRESS: at 86.55% examples, 385152 words/s, in_qsize 17, out_qsize 0\n",
      "2019-03-28 00:25:51,754 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-28 00:25:51,794 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-28 00:25:51,803 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-28 00:25:51,806 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-28 00:25:51,819 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-28 00:25:51,821 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-28 00:25:51,841 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 00:25:51,845 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 00:25:51,847 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 00:25:51,853 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 00:25:51,859 : INFO : EPOCH - 4 : training on 1213051 raw words (907260 effective words) took 2.2s, 408411 effective words/s\n",
      "2019-03-28 00:25:53,007 : INFO : EPOCH 5 - PROGRESS: at 47.09% examples, 387786 words/s, in_qsize 18, out_qsize 3\n",
      "2019-03-28 00:25:54,037 : INFO : EPOCH 5 - PROGRESS: at 92.29% examples, 393419 words/s, in_qsize 10, out_qsize 0\n",
      "2019-03-28 00:25:54,043 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-28 00:25:54,051 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-28 00:25:54,095 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-28 00:25:54,097 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-28 00:25:54,108 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-28 00:25:54,113 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-28 00:25:54,116 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 00:25:54,123 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 00:25:54,125 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 00:25:54,135 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 00:25:54,145 : INFO : EPOCH - 5 : training on 1213051 raw words (907295 effective words) took 2.2s, 405844 effective words/s\n",
      "2019-03-28 00:25:55,246 : INFO : EPOCH 6 - PROGRESS: at 37.16% examples, 329665 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-28 00:25:56,309 : INFO : EPOCH 6 - PROGRESS: at 82.39% examples, 359306 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-28 00:25:56,464 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-28 00:25:56,491 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-28 00:25:56,513 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-28 00:25:56,526 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-28 00:25:56,532 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-28 00:25:56,564 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-28 00:25:56,566 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 00:25:56,572 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 00:25:56,578 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 00:25:56,589 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 00:25:56,591 : INFO : EPOCH - 6 : training on 1213051 raw words (907604 effective words) took 2.4s, 384213 effective words/s\n",
      "2019-03-28 00:25:57,672 : INFO : EPOCH 7 - PROGRESS: at 44.57% examples, 392851 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-28 00:25:58,574 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-28 00:25:58,606 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-28 00:25:58,615 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-28 00:25:58,619 : INFO : worker thread finished; awaiting finish of 6 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-28 00:25:58,621 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-28 00:25:58,624 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-28 00:25:58,634 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 00:25:58,637 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 00:25:58,649 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 00:25:58,653 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 00:25:58,656 : INFO : EPOCH - 7 : training on 1213051 raw words (907199 effective words) took 2.0s, 451267 effective words/s\n",
      "2019-03-28 00:25:59,726 : INFO : EPOCH 8 - PROGRESS: at 43.77% examples, 389093 words/s, in_qsize 19, out_qsize 2\n",
      "2019-03-28 00:26:00,624 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-28 00:26:00,692 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-28 00:26:00,698 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-28 00:26:00,704 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-28 00:26:00,707 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-28 00:26:00,708 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-28 00:26:00,727 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 00:26:00,735 : INFO : EPOCH 8 - PROGRESS: at 98.33% examples, 440756 words/s, in_qsize 1, out_qsize 3\n",
      "2019-03-28 00:26:00,744 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 00:26:00,746 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 00:26:00,758 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 00:26:00,763 : INFO : EPOCH - 8 : training on 1213051 raw words (907841 effective words) took 2.1s, 441919 effective words/s\n",
      "2019-03-28 00:26:01,835 : INFO : EPOCH 9 - PROGRESS: at 45.46% examples, 404275 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-28 00:26:02,833 : INFO : EPOCH 9 - PROGRESS: at 89.04% examples, 400197 words/s, in_qsize 14, out_qsize 0\n",
      "2019-03-28 00:26:02,865 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-28 00:26:02,912 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-28 00:26:02,923 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-28 00:26:02,929 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-28 00:26:02,955 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-28 00:26:02,972 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-28 00:26:02,987 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 00:26:02,993 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 00:26:02,996 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 00:26:03,025 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 00:26:03,027 : INFO : EPOCH - 9 : training on 1213051 raw words (906844 effective words) took 2.2s, 410911 effective words/s\n",
      "2019-03-28 00:26:04,100 : INFO : EPOCH 10 - PROGRESS: at 32.24% examples, 289683 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-28 00:26:05,104 : INFO : EPOCH 10 - PROGRESS: at 73.36% examples, 331121 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-28 00:26:05,522 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-28 00:26:05,536 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-28 00:26:05,540 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-28 00:26:05,544 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-28 00:26:05,551 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-28 00:26:05,553 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-28 00:26:05,561 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-28 00:26:05,576 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-28 00:26:05,590 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-28 00:26:05,597 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-28 00:26:05,603 : INFO : EPOCH - 10 : training on 1213051 raw words (908139 effective words) took 2.5s, 361821 effective words/s\n",
      "2019-03-28 00:26:05,608 : INFO : training on a 12130510 raw words (9075187 effective words) took 23.0s, 394861 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9075187, 12130510)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(sentences, total_examples=len(sentences), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s get to the fun stuff already! Since we trained on user reviews, it would be nice to see similarity on some adjectives. This first example shows a simple look up of words similar to the word 'young'. All we need to do here is to call the most_similar function and provide the word 'young' as the positive example. This returns the top 10 similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-28 00:26:14,048 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('gay', 0.6983577013015747),\n",
       " ('teen', 0.6771925687789917),\n",
       " ('female', 0.6601540446281433),\n",
       " ('adults', 0.6504086852073669),\n",
       " ('sexually', 0.632117748260498),\n",
       " ('transgender', 0.6239749193191528),\n",
       " ('male', 0.6151970624923706),\n",
       " ('children', 0.6108031272888184),\n",
       " ('kids', 0.6039509177207947),\n",
       " ('teenage', 0.60386061668396)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"young\"\n",
    "model.wv.most_similar(positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('opinions', 0.7887747287750244),\n",
       " ('stance', 0.7273621559143066),\n",
       " ('assyrian', 0.7138534784317017),\n",
       " ('perspctive', 0.6914175748825073),\n",
       " ('proudest', 0.6599768996238708),\n",
       " ('favourite', 0.6536139845848083),\n",
       " ('midfoot', 0.6442692279815674),\n",
       " ('zahra', 0.6429859399795532),\n",
       " ('arabidopsis', 0.6400624513626099),\n",
       " ('whatdid', 0.6365878582000732)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = \"opinion\"\n",
    "model.wv.most_similar(positive=w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crimes', 0.7659684419631958),\n",
       " ('committed', 0.7128411531448364),\n",
       " ('accusations', 0.7062324285507202),\n",
       " ('murder', 0.6925055384635925),\n",
       " ('israel', 0.6859065294265747),\n",
       " ('treason', 0.684887707233429),\n",
       " ('syria', 0.6803385019302368),\n",
       " ('corruption', 0.6714112758636475),\n",
       " ('party', 0.6637689471244812),\n",
       " ('congress', 0.6575760841369629)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = \"crime\"\n",
    "model.wv.most_similar(positive=w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you could even use Word2Vec to compute similarity between two words in the vocabulary by invoking the \"similarity(...)\" function and passing in the relevant words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5480313677944857"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity between two different words\n",
    "model.wv.similarity(w1=\"spain\", w2=\"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6540631693632291"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity between two different words\n",
    "model.wv.similarity(w1=\"germany\", w2=\"countries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7821647513259993"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity between two different words\n",
    "model.wv.similarity(w1=\"trump\", w2=\"president\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1781629686372547"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity between two different words\n",
    "model.wv.similarity(w1=\"learning\", w2=\"treason\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the word vectors for creating features of our text dataset.\n",
    "We may draw on the assumption that sentences that convey \"insincere\" content are composed of words that are, contextually, more similar to terms like \"treason\", \"crime\", \"sabotage\", \"terrorist\".\n",
    "For each sentence, we can then calculate the average similarity to these words.\n",
    "To verify whether this approach provides any informational value, we compare the average similarity of \"insincere\" sentences with the mentioned words to that of \"sincere\" sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      0                                                  1  \\\n",
      "0                   qid                                      question_text   \n",
      "1  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
      "2  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
      "3  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
      "4  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
      "5  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
      "6  00004f9a462a357c33be  Is Gaza slowly becoming Auschwitz, Dachau or T...   \n",
      "7  00005059a06ee19e11ad  Why does Quora automatically ban conservative ...   \n",
      "8  0000559f875832745e2e  Is it crazy if I wash or wipe my groceries off...   \n",
      "9  00005bd3426b2d0c8305  Is there such a thing as dressing moderately, ...   \n",
      "\n",
      "        2  similarity_treason  similarity_crime  similarity_sabotage  \\\n",
      "0  target                 NaN               NaN                  NaN   \n",
      "1       0                 NaN               NaN                  NaN   \n",
      "2       0                 NaN               NaN                  NaN   \n",
      "3       0                 NaN               NaN                  NaN   \n",
      "4       0                 NaN               NaN                  NaN   \n",
      "5       0                 NaN               NaN                  NaN   \n",
      "6       0                 NaN               NaN                  NaN   \n",
      "7       0                 NaN               NaN                  NaN   \n",
      "8       0                 NaN               NaN                  NaN   \n",
      "9       0                 NaN               NaN                  NaN   \n",
      "\n",
      "   similarity_terrorist  \n",
      "0                   NaN  \n",
      "1                   NaN  \n",
      "2                   NaN  \n",
      "3                   NaN  \n",
      "4                   NaN  \n",
      "5                   NaN  \n",
      "6                   NaN  \n",
      "7                   NaN  \n",
      "8                   NaN  \n",
      "9                   NaN  \n"
     ]
    }
   ],
   "source": [
    "#import pandas and numpy packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the words related to \"insincere\" context that we will use for feature creation\n",
    "insincere_topics = [\"treason\", \"crime\", \"sabotage\", \"terrorist\"]\n",
    "\n",
    "# Create DataFrame from train_raw\n",
    "df_train_raw = pd.DataFrame(train_raw)\n",
    "# Add columns with nan values to later store similarity values\n",
    "# Number of columns will equal the number of words used for determining the similarity to \"insincere\" content\n",
    "for i in range (0, len(insincere_topics)):\n",
    "    df_train_raw['similarity_'+insincere_topics[i]] = np.nan\n",
    "print (df_train_raw[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0 of len(train_raw) = 100000\n",
      "i = 5000 of len(train_raw) = 100000\n",
      "i = 10000 of len(train_raw) = 100000\n",
      "i = 15000 of len(train_raw) = 100000\n",
      "i = 20000 of len(train_raw) = 100000\n",
      "i = 25000 of len(train_raw) = 100000\n",
      "i = 30000 of len(train_raw) = 100000\n",
      "i = 35000 of len(train_raw) = 100000\n",
      "i = 40000 of len(train_raw) = 100000\n",
      "i = 45000 of len(train_raw) = 100000\n",
      "i = 50000 of len(train_raw) = 100000\n",
      "i = 55000 of len(train_raw) = 100000\n",
      "i = 60000 of len(train_raw) = 100000\n",
      "i = 65000 of len(train_raw) = 100000\n",
      "i = 70000 of len(train_raw) = 100000\n",
      "i = 75000 of len(train_raw) = 100000\n",
      "i = 80000 of len(train_raw) = 100000\n",
      "i = 85000 of len(train_raw) = 100000\n",
      "i = 90000 of len(train_raw) = 100000\n",
      "i = 95000 of len(train_raw) = 100000\n",
      "                          0  \\\n",
      "99984  1394ce4e03490702a6b1   \n",
      "99985  1394d4ce23b18714e726   \n",
      "99986  1394d75405f6320d07e5   \n",
      "99987  1394e8fb0e3382faf27e   \n",
      "99988  1394ef966dca965819b3   \n",
      "99989  13951b7e70572214e93d   \n",
      "99990  13951e98d97449e53c48   \n",
      "99991  139528d12e5463ab2eab   \n",
      "99992  13952e88c5a74c5ebc11   \n",
      "99993  13953da020391ec98e20   \n",
      "99994  139548bcb548e796aa2f   \n",
      "99995  13955494b7a10bbdd99e   \n",
      "99996  13956982bc38015ca63f   \n",
      "99997  1395736e4b14323fc16a   \n",
      "99998  139577538cc474df1aa0   \n",
      "\n",
      "                                                       1  2  \\\n",
      "99984  Which jewellery shop is good to buy good quali...  0   \n",
      "99985  Is there any direct relation between rice and ...  0   \n",
      "99986                    Are gyms becoming more popular?  0   \n",
      "99987  Can Btech students with branch mechanical and ...  0   \n",
      "99988  Which are the two cheap countries in Europe to...  0   \n",
      "99989  What impact did playing the Ayaan Ahmed Khan's...  0   \n",
      "99990  What are some ways people mistake a bad idea a...  0   \n",
      "99991      How important are personal boundaries to you?  0   \n",
      "99992  Can elliptic functions allow for an explicit s...  0   \n",
      "99993                    Who founded Konoha from Naruto?  0   \n",
      "99994             What are some plot holes in One Piece?  0   \n",
      "99995         How do I surprise my love on his birthday?  0   \n",
      "99996  What is it like to switch from Adobe Premiere ...  0   \n",
      "99997  What will be the effect of long term capital g...  0   \n",
      "99998  Which is greater, the need to eat or have sex?...  0   \n",
      "\n",
      "       similarity_treason  similarity_crime  similarity_sabotage  \\\n",
      "99984            0.032402          0.031998            -0.082786   \n",
      "99985            0.121282          0.090066            -0.008866   \n",
      "99986            0.112670          0.135797             0.027804   \n",
      "99987            0.019759          0.001676            -0.101808   \n",
      "99988            0.080826          0.168035            -0.036301   \n",
      "99989            0.172002          0.072004             0.087397   \n",
      "99990            0.069033          0.089752            -0.012948   \n",
      "99991            0.015454          0.034586            -0.021178   \n",
      "99992            0.172981          0.073348            -0.030603   \n",
      "99993            0.217805          0.134923            -0.002208   \n",
      "99994            0.059450          0.038412            -0.081652   \n",
      "99995            0.123600          0.053232             0.059642   \n",
      "99996            0.051770         -0.022837            -0.017558   \n",
      "99997            0.134264          0.149047             0.053170   \n",
      "99998            0.084240          0.102818            -0.001179   \n",
      "\n",
      "       similarity_terrorist  \n",
      "99984             -0.058431  \n",
      "99985              0.091496  \n",
      "99986              0.119781  \n",
      "99987             -0.056920  \n",
      "99988              0.160601  \n",
      "99989              0.100203  \n",
      "99990              0.021726  \n",
      "99991             -0.009920  \n",
      "99992              0.069753  \n",
      "99993              0.164403  \n",
      "99994              0.023381  \n",
      "99995              0.055945  \n",
      "99996             -0.001262  \n",
      "99997              0.135132  \n",
      "99998              0.084944  \n"
     ]
    }
   ],
   "source": [
    "# Go through the whole list of list \"sentences\" and provide similarity values for each of the words with the mentioned topics.\n",
    "# Begin with the first occurence of a \"1\" in train_raw[i][2], break the loop after\n",
    "stop_index = 100\n",
    "### for i in range (50, 56):\n",
    "for i in range (0, len(train_raw)):\n",
    "    if (i % 5000 == 0):\n",
    "        print (\"i =\", i, \"of len(train_raw) =\", len(train_raw))\n",
    "    if (train_raw[i][2] == \"1\" or \"0\"):\n",
    "        ### print (sentences[i])\n",
    "        ### stop_index = i\n",
    "        ### print (\"stop_index: \", stop_index)\n",
    "        # Calculate similarity of every word of the sentence to every word of the insincere_topics.\n",
    "        # Add up all the similarities, and then build the average.\n",
    "        # Determine the number of words of the sentence for that.\n",
    "        number_of_words_in_sentence = len(sentences[i])\n",
    "        if (number_of_words_in_sentence == 0):\n",
    "            number_of_words_in_sentence = 1\n",
    "        for k in range (0, len(insincere_topics)):\n",
    "            tmp_similarity_sum = 0\n",
    "            w1 = insincere_topics[k]\n",
    "            for j in range (0, len(sentences[i])):\n",
    "                w2 = sentences[i][j]\n",
    "                # Get similarity of word to current insincere_topic\n",
    "                tmp_similarity = model.wv.similarity(w1, w2)\n",
    "                ### print (\"tmp_similarity = model.wv.similarity(\", w1, \",\", w2,\") = \", tmp_similarity)\n",
    "                tmp_similarity_sum = tmp_similarity_sum + tmp_similarity\n",
    "                ### print (\"new tmp_similarity_sum = \", tmp_similarity_sum)\n",
    "            tmp_similarity_average = 0\n",
    "            tmp_similarity_average = tmp_similarity_sum / number_of_words_in_sentence\n",
    "            # Determine which column in dataframe records similarities for given word\n",
    "            tmp_similarity_column_number = df_train_raw.columns.get_loc('similarity_'+insincere_topics[k])\n",
    "            # Save average similarity value there\n",
    "            df_train_raw.iloc[i, tmp_similarity_column_number] = tmp_similarity_average\n",
    "        ### print (\"IF i = \", i , \" > stop_index + 2 = \", stop_index, \"+ 2 --> break\")\n",
    "    ### if (i > stop_index + 2):\n",
    "        ### break\n",
    "\n",
    "print (df_train_raw[i-15:i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create DataFrames that contain\n",
    "0) only those sentences that were NOT labelled as insincere\n",
    "1) only those sentences that were labelled as insincere\n",
    "\n",
    "Then, we calculate the average similarity of those sentences to the \"insincere_topics\" depending on the abovementioned label.\n",
    "If the average similarity to the \"insincere_topics\" is usually considerably higher for those sentences labeled as \"insincere\", then it might be useful to use this similarity as a feature for our model (the model that shall determine whether a sentence had been insincere or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insincere_topic:  treason\n",
      "df_train_raw_target_1['similarity_ treason ] =  0.1965593864255121\n",
      "df_train_raw_target_0['similarity_ treason ] =  0.10510808685340248\n",
      "-------------------------\n",
      "insincere_topic:  crime\n",
      "df_train_raw_target_1['similarity_ crime ] =  0.1858277078470125\n",
      "df_train_raw_target_0['similarity_ crime ] =  0.08734700818744957\n",
      "-------------------------\n",
      "insincere_topic:  sabotage\n",
      "df_train_raw_target_1['similarity_ sabotage ] =  0.04299316762722095\n",
      "df_train_raw_target_0['similarity_ sabotage ] =  0.009735614315394332\n",
      "-------------------------\n",
      "insincere_topic:  terrorist\n",
      "df_train_raw_target_1['similarity_ terrorist ] =  0.1895742749255517\n",
      "df_train_raw_target_0['similarity_ terrorist ] =  0.07829692343517229\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "df_train_raw_target_1 = df_train_raw[df_train_raw[2] == '1']\n",
    "df_train_raw_target_0 = df_train_raw[df_train_raw[2] == '0']\n",
    "\n",
    "for i in range (0, len(insincere_topics)):\n",
    "    print (\"insincere_topic: \", insincere_topics[i])\n",
    "    print (\"df_train_raw_target_1['similarity_\", insincere_topics[i], \"] = \", df_train_raw_target_1['similarity_'+insincere_topics[i]].mean())\n",
    "    print (\"df_train_raw_target_0['similarity_\", insincere_topics[i], \"] = \", df_train_raw_target_0['similarity_'+insincere_topics[i]].mean())\n",
    "    print (\"-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results - average similarity to the \"insincere_topics\" is usually considerably higher for those sentences labeled as \"insincere\" - support the assumption that it might be useful to use this similarity as features for our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
